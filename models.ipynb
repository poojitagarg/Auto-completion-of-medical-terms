{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Auto_Completion_of_medicine_terms.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hU0sJJoRlSQ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
        "from keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmXc-eI1qHra"
      },
      "source": [
        "train_data_source = './drug.names.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_data_source, header=None)\n",
        "\n",
        "# convert string to lower case\n",
        "train_texts = train_df[1].values\n",
        "train_texts = [s.lower() for s in train_texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Fc1kSv8rynZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ebdf25e-4641-46a1-aa06-5e55cf65eece"
      },
      "source": [
        "# data cleaning to remove dosage amounts in medicine data\n",
        "x=[]\n",
        "for i in range(len(train_texts)):\n",
        "  s=train_texts[i].split()\n",
        "  x.append(s[0])\n",
        "x=pd.DataFrame(x)\n",
        "train_texts = x[0].values\n",
        "train_texts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['omeprazole_cap', 'dressit', 'flaminal', ..., 'coloplast_assura',\n",
              "       'slow-fe_tab', 'sure-amp_bupivac'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZH4SlnGq-dS"
      },
      "source": [
        "# =======================Convert string to index================\n",
        "# Tokenizer\n",
        "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "tk.fit_on_texts(train_texts)\n",
        "# If we already have a character list, then replace the tk.word_index\n",
        "# If not, just skip below part\n",
        "\n",
        "# construct a new vocabulary\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz 0123456789,;.!?:'\\\"/\\\\|_@#%^&*~`+-=<>()[]{}\"\n",
        "char_len = len(alphabet)\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "char_dict['$'] = 0\n",
        "\n",
        "# Use char_dict to replace the tk.word_index\n",
        "tk.word_index = char_dict.copy()\n",
        "# Add 'UNK' to the vocabulary\n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
        "\n",
        "# Convert string to index\n",
        "train_sequences = tk.texts_to_sequences(train_texts)\n",
        "\n",
        "# Padding\n",
        "train_data = pad_sequences(train_sequences, maxlen=50, padding='post')\n",
        "\n",
        "# Convert to numpy array\n",
        "train_data = np.array(train_data, dtype='float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxj0KtGl9t-5"
      },
      "source": [
        "# creating inverse dictionary for getting outputs\n",
        "inverse_dict = {}\n",
        "for i,char in enumerate(alphabet):\n",
        "  inverse_dict[i+1] = char\n",
        "inverse_dict[0] = '$'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPl4f9Hq9uP0"
      },
      "source": [
        "# one hot encoding of singular char, a string and a position\r\n",
        "\r\n",
        "def one_hot(val,len=char_len+1):\r\n",
        "  temp_list = []\r\n",
        "  for var in val:\r\n",
        "    temp = np.zeros(len,dtype=int)\r\n",
        "    temp[int(var)] = 1\r\n",
        "    temp_list.append(temp)\r\n",
        "  return temp_list\r\n",
        "\r\n",
        "def one_hot_char(val,len=char_len+1):\r\n",
        "  temp = np.zeros(len,dtype=int)\r\n",
        "  temp[char_dict[val]] = 1\r\n",
        "  return np.array(temp)\r\n",
        "\r\n",
        "def one_hot_value(pos,len=char_len+1):\r\n",
        "  temp = np.zeros(len,dtype=int)\r\n",
        "  temp[int(pos)] = 1\r\n",
        "  return np.array(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN-pJ8g7qjqR"
      },
      "source": [
        "# prepairing datasets for training\n",
        "def data_prep(dataset,lookback):\n",
        "  train_x = []\n",
        "  train_y = []\n",
        "  for i in range(0,dataset.shape[0]):\n",
        "    for j in range(lookback-1,50):\n",
        "      if(dataset[i,j] != 0):\n",
        "        temp_list = one_hot(dataset[i,j-lookback+1:j+1])\n",
        "        train_x.append(temp_list)\n",
        "        temp_list2 = np.zeros(char_len+1,dtype = int)\n",
        "        temp_list2[int(dataset[i,j+1])] = 1\n",
        "        train_y.append(temp_list2)\n",
        "      else:\n",
        "        break;\n",
        "  return np.array(train_x), np.array(train_y)\n",
        "\n",
        "def data_prep2(dataset,lookback):\n",
        "  train_x = []\n",
        "  train_y = []\n",
        "  for i in range(0,dataset.shape[0]):\n",
        "    if(dataset[i,lookback-1]!= 0):\n",
        "      temp_list = one_hot(dataset[i,:lookback])\n",
        "      train_x.append(temp_list)\n",
        "      temp_list2 = np.zeros(char_len+1,dtype = int)\n",
        "      temp_list2[int(dataset[i,lookback])] = 1\n",
        "      train_y.append(temp_list2)\n",
        "  return np.array(train_x), np.array(train_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K7IJ2xoNS-N"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Embedding\n",
        "import keras.models\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "# Building Neural Network model\n",
        "def build_model(n_shape,l1=100,l2=100,d1=0.4,d2=0.4,len=char_len+1):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(l1,input_shape= n_shape,return_sequences=True,activation='tanh'))\n",
        "  model.add(Dropout(d1))\n",
        "  model.add(LSTM(l2,return_sequences=False,activation='tanh'))\n",
        "  model.add(Dropout(d2))\n",
        "  model.add(Dense(1024, activation='relu'))\n",
        "  model.add(Dense(256,activation='relu'))\n",
        "  model.add(Dense(len, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def model_fit(model,train_x,train_y,bs=32,ep=100,shf=False):\n",
        "  hist = model.fit(train_x,train_y,batch_size = bs,epochs = ep,shuffle = shf)\n",
        "  return hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDWJX4d8Bqr4"
      },
      "source": [
        "# Building models for input 3-8 characters\n",
        " models = []\n",
        "train_x = []\n",
        "train_y = []\n",
        "for i in range(3,8):\n",
        "  temp1, temp2 = data_prep2(train_data,i)\n",
        "  train_x.append(temp1)\n",
        "  train_y.append(temp2)\n",
        "  models.append(build_model(n_shape = (train_x[i-3].shape[1],train_x[i-3].shape[2])))\n",
        "\n",
        "temp1, temp2 = data_prep(train_data,8)\n",
        "train_x.append(temp1)\n",
        "train_y.append(temp2)\n",
        "models.append(build_model(n_shape = (train_x[5].shape[1],train_x[5].shape[2])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIiSazWMIPY3"
      },
      "source": [
        "hist_3words = model_fit(models[0],train_x[0],train_y[0])\n",
        "keras.models.save_model(models[0],\"model_3word.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajnQ102jIPUl"
      },
      "source": [
        "hist_4words = model_fit(models[1],train_x[1],train_y[1])\n",
        "keras.models.save_model(models[1],\"model_4word.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpwCdL3LHkGa"
      },
      "source": [
        "hist_5words = model_fit(models[2],train_x[2],train_y[2])\n",
        "keras.models.save_model(models[2],\"model_5word.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohdBnL72HkEh"
      },
      "source": [
        "hist_6words = model_fit(models[3],train_x[3],train_y[3])\n",
        "keras.models.save_model(models[3],\"model_6word.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qunv2zIHj8d"
      },
      "source": [
        "hist_7words = model_fit(models[4],train_x[4],train_y[4])\n",
        "keras.models.save_model(models[4],\"model_7word.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrvLeCmOIjAd"
      },
      "source": [
        "hist_8words = model_fit(models[5],train_x[5],train_y[5])\n",
        "keras.models.save_model(models[5],\"model_8word.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bOiMYLasrJs",
        "outputId": "b0ff2a70-998b-47bb-8ba5-977e1c6929e8"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_8 (LSTM)                (None, 3, 100)            68000     \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 3, 100)            0         \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1024)              103424    \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 69)                17733     \n",
            "=================================================================\n",
            "Total params: 531,957\n",
            "Trainable params: 531,957\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk-5TNBeBqhD"
      },
      "source": [
        "# loading trained models (if any)\n",
        "\n",
        "word_predict = []\n",
        "word_predict.append(keras.models.load_model(\"model_3word.h5\"))\n",
        "word_predict.append(keras.models.load_model(\"model_4word.h5\"))\n",
        "word_predict.append(keras.models.load_model(\"model_5word.h5\"))\n",
        "word_predict.append(keras.models.load_model(\"model_6word.h5\"))\n",
        "word_predict.append(keras.models.load_model(\"model_7word.h5\"))\n",
        "word_predict.append(keras.models.load_model(\"model_8word.h5\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzui8EGBrBta"
      },
      "source": [
        "# Creating fuctions to predict output\n",
        "\n",
        "# Getting the raw charactrs and converting to one hot enchoded character numpy array\n",
        "def Vectorize(word):\n",
        "  word = word.lower()\n",
        "  length = len(word)\n",
        "  if(length<3):\n",
        "    print(\"Enter more letters\")\n",
        "    return np.zeros((1,1))\n",
        "  word_arr = []\n",
        "  for i in word:\n",
        "    word_arr.append(char_dict[i])\n",
        "  word_vect = np.array(one_hot(word_arr))\n",
        "  return np.reshape(word_vect,(1,word_vect.shape[0],word_vect.shape[1]))\n",
        "\n",
        "def one_hot_output(vect):\n",
        "  max = 0\n",
        "  max_pos = -1\n",
        "  eof = False\n",
        "  for i,val in enumerate(np.reshape(vect,(vect.shape[1],))):\n",
        "    if(val>0.5):\n",
        "      if(i == 0):\n",
        "        eof=True\n",
        "      return one_hot_value(i),eof\n",
        "    elif(max>val):\n",
        "      max = val\n",
        "      max_pos = i\n",
        "    if(max_pos==0):\n",
        "      eof = True\n",
        "  return one_hot_value(max_pos),eof\n",
        "\n",
        "# predicting next character while looping and checking for end of char or max word length\n",
        "def word_pred(word_vect):\n",
        "  if(word_vect.shape[1] == 1):\n",
        "    return np.zeros((1,1))\n",
        "  eof = False\n",
        "  if(word_vect.shape[1]<8):\n",
        "    len_ = word_vect.shape[1]\n",
        "    for i in range(len_,8):\n",
        "      next_word,eof = one_hot_output(word_predict[i-3].predict(word_vect))\n",
        "      word_vect = np.append(word_vect,np.reshape(next_word,(1,1,char_len+1)),axis=1)\n",
        "      if(eof):\n",
        "        return word_vect\n",
        "  while( (not eof) and (word_vect.shape[1]<50)):\n",
        "    next_word,eof = one_hot_output(word_predict[5].predict(word_vect[:,-8:,:]))\n",
        "    word_vect = np.append(word_vect,np.reshape(next_word,(1,1,char_len+1)),axis=1)\n",
        "  return word_vect\n",
        "\n",
        "# Convert one hot encoded word to readable characters\n",
        "def deencode(one_vect):\n",
        "  for i,val in enumerate(one_vect):\n",
        "    if(val==1):\n",
        "      return inverse_dict[i]\n",
        "\n",
        "def decode(vect):\n",
        "  if(vect.shape[1] == 1):\n",
        "    return np.zeros((1,1))\n",
        "  word = \"\"\n",
        "  for i in range(vect.shape[1]):\n",
        "    word += deencode(np.reshape(vect[:,i,:],(vect.shape[2])))\n",
        "  return word\n",
        "\n",
        "def Prediction(word):\n",
        "  #Convert original text to Vector by one hot encoding\n",
        "  word_vect = Vectorize(word)\n",
        "\n",
        "  #Predict the Output Vector using Deep Learning Models\n",
        "  output_vect = word_pred(word_vect)\n",
        "\n",
        "  #Convert the Output Vector to Human Redable Word\n",
        "  actual_word = decode(output_vect)\n",
        "  \n",
        "  return actual_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oTAjYwpcrTgW",
        "outputId": "5bdcaba7-ef33-4c9d-d873-f6f8277dc94d"
      },
      "source": [
        "Prediction(\"ran\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ranitidine$'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8x5ca77oiRUB",
        "outputId": "224c3d23-ae1f-46a0-c09c-26d1b166066d"
      },
      "source": [
        "Prediction(\"Inda\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'indapamide_liq$'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PIXvfd8CiV8b",
        "outputId": "3506ae31-24f7-4f3c-ab43-adcc7cf95beb"
      },
      "source": [
        "Prediction(\"Amil\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'amiloride$'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "u6ImjWeCiVa9",
        "outputId": "155d33e6-b69f-4a8c-d8a5-7681682062c4"
      },
      "source": [
        "Prediction(\"Peri\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'perindopril$'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OmHRJn6ej44E",
        "outputId": "6c4b96c3-68ee-4c9c-8f51-741cc54439ad"
      },
      "source": [
        "Prediction(\"mal\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'malarone_}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-QOy5be5fFo"
      },
      "source": [
        "# Multiple-output-prediction\r\n",
        "\r\n",
        "def one_hot_output_n(vect,n_pred_left):\r\n",
        "  \r\n",
        "  eof = []\r\n",
        "\r\n",
        "  for i in range(n_pred_left):\r\n",
        "    eof.append(False)\r\n",
        "\r\n",
        "  word_val = []\r\n",
        "  rem = n_pred_left\r\n",
        "  temp = np.reshape(vect,(vect.shape[1],))\r\n",
        "  index_list = np.argsort(temp)\r\n",
        "  index_list = index_list.tolist()\r\n",
        "  index_list.reverse()\r\n",
        "  first = temp[index_list[0]]\r\n",
        "\r\n",
        "  for i in index_list:\r\n",
        "    if(temp[i]>0.7):\r\n",
        "      if(i == 0):\r\n",
        "        eof[0] = True\r\n",
        "      word_val.append(one_hot_value(i))\r\n",
        "      return word_val,eof\r\n",
        "\r\n",
        "    elif ((rem != 0) and (first - temp[i] <= .3)):\r\n",
        "      word_val.append(one_hot_value(i))\r\n",
        "      if(i == 0):\r\n",
        "        eof[n_pred_left-rem] = True\r\n",
        "      rem = rem-1\r\n",
        "\r\n",
        "    else:\r\n",
        "      break\r\n",
        "\r\n",
        "  return word_val,eof\r\n",
        "\r\n",
        "# get all possible character outcomes with 30% difference to original predicted character\r\n",
        "def get_possib(word_vect,n_pred):\r\n",
        "\r\n",
        "  t_list = []\r\n",
        "  if(word_vect.shape[1] == 1):\r\n",
        "    return np.zeros((1,1))\r\n",
        "  first = True\r\n",
        "  rem = 2\r\n",
        "  if(word_vect.shape[1]<8):\r\n",
        "    len_ = word_vect.shape[1]\r\n",
        "\r\n",
        "    for i in range(len_,8):\r\n",
        "      next_word_list,eof_list = one_hot_output_n(word_predict[i-3].predict(word_vect),2)\r\n",
        "\r\n",
        "      if(first):\r\n",
        "        t_list.append(np.append(word_vect,np.reshape(next_word_list[0],(1,1,char_len+1)),axis=1))\r\n",
        "        first = False\r\n",
        "\r\n",
        "      if(len(next_word_list)>1 and rem>0):\r\n",
        "        t_list.append(np.append(word_vect,np.reshape(next_word_list[1],(1,1,char_len+1)),axis=1))\r\n",
        "        rem = rem-1\r\n",
        "\r\n",
        "      word_vect = np.append(word_vect,np.reshape(next_word_list[0],(1,1,char_len+1)),axis=1)\r\n",
        "      t_list[0] = word_vect\r\n",
        "\r\n",
        "      if(eof_list[0]):\r\n",
        "        return t_list\r\n",
        "\r\n",
        "  while( (not eof_list[0]) and (word_vect.shape[1]<50)):\r\n",
        "    next_word_list,eof_list = one_hot_output_n(word_predict[5].predict(word_vect[:,-8:,:]),2)\r\n",
        "\r\n",
        "    if(first):\r\n",
        "      t_list.append(np.append(word_vect,np.reshape(next_word_list[0],(1,1,char_len+1)),axis=1))\r\n",
        "      first = False\r\n",
        "\r\n",
        "    if(len(next_word_list)>1 and rem>0):\r\n",
        "      t_list.append(np.append(word_vect,np.reshape(next_word_list[1],(1,1,char_len+1)),axis=1))\r\n",
        "      rem=rem-1\r\n",
        "\r\n",
        "    word_vect = np.append(word_vect,np.reshape(next_word_list[0],(1,1,char_len+1)),axis=1)\r\n",
        "    t_list[0] = word_vect\r\n",
        "\r\n",
        "  return t_list\r\n",
        "\r\n",
        "\r\n",
        "def word_pred_n(vect,n_pred):\r\n",
        "\r\n",
        "  temp_list = get_possib(vect,n_pred)\r\n",
        "  words = []\r\n",
        "  words.append(temp_list[0])\r\n",
        "\r\n",
        "  for i in range(1,len(temp_list)):\r\n",
        "    words.append(word_pred(temp_list[i]))\r\n",
        "\r\n",
        "  return words\r\n",
        "\r\n",
        "def Prediction_n(word,n_pred=3):\r\n",
        "\r\n",
        "  if(n_pred<1):\r\n",
        "    return []\r\n",
        "\r\n",
        "  # Convert original text to Vector by one hot encoding\r\n",
        "  word_vect = Vectorize(word)\r\n",
        "\r\n",
        "  # Predict the Output Vector using Deep Learning Models\r\n",
        "\r\n",
        "  output_vect = word_pred_n(word_vect,n_pred)\r\n",
        "\r\n",
        "  # Convert all the Output Vectors to Human Redable Words\r\n",
        "  actual_words = []\r\n",
        "  for i in range(len(output_vect)):\r\n",
        "    actual_words.append(decode(output_vect[i]))\r\n",
        "  \r\n",
        "  return actual_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnSFiVSAfhjY",
        "outputId": "877d4463-ef35-404f-a10f-796e981070a4"
      },
      "source": [
        "Prediction_n('dicy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dicynene_inj$', 'dicycloverine$', 'dicynene_tab$']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}